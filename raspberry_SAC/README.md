# SAC-Based Autonomous Driving on a Real-World Robotic Vehicle

This phase represents the final implementation stage, transitioning from simulated autonomous driving to real-world robot deployment. It builds upon the previously successful SAC + VAE framework established in the simulation phase, adapting and optimizing it for a physical robotic vehicle.

## Overview

In this phase, a fully autonomous robotic vehicle is developed and trained to navigate real-world track layouts. Key objectives include:

- **Real-time inference and decision-making** using compact latent representations generated by a pretrained VAE.
- **Smooth and precise vehicle control** leveraging continuous steering and acceleration commands via SAC.
- **Adaptation to real-world sensor input**, overcoming the simulation-to-reality gap.
- **Robust performance** in varying lighting conditions and real-world physical constraints.
- **New robot design** to improve training efficiency, mantainability, stability and minimize noise, errors and failures (or at least try it).

The vehicle consists of carefully selected hardware components designed to optimize performance, maintainability, and reliability in real-world scenarios.

## Watch the Final Robot in Action

https://github.com/user-attachments/assets/5dc00ee3-4cc8-4906-854c-d2a1b5d1aa3b

## Hardware Design

### Processor: Raspberry Pi 3B+

- Sufficient computational power for real-time inference (>10 FPS).
- Extensive community support and readily available resources.

### Motor Controller: L298N

- Wide voltage support (up to 12–35 V), ensuring robustness against voltage variations.
- Heat dissipation capabilities, ensuring reliable operation under varying current loads.

### Control Interface: Arduino

- Seamless interfacing with 5V logic sensors (IR sensors).
- Offloading sensor polling and basic motor control loops from Raspberry Pi.

### Power Source: Nickel-Metal Hydride (NiMH) Batteries

- Enhanced safety and reduced risk compared to Li-ion batteries.

### Communication

- **Serial communication** between Raspberry Pi and Arduino for simplicity, reliability, and ease of debugging.
- **Wi-Fi** for communication between Raspberry Pi and the controlling PC, ensuring wireless monitoring and command transmission.

### Sensors and Actuators

- **Motors:** Two KKmoon motor-wheel sets for independent control on each side.
- **Front wheel:** A single free-rotating wheel that, despite its carefree spinning, actually keeps things surprisingly predictable, proving that sometimes less really is more when it comes to stable, consistent steering.
- **Camera Modules:**
  - **Raspberry Pi Camera Module V2:** 480p/90fps, 62º viewing angle.
  - **Wide-angle camera module:** Same sensor, 160º viewing angle lens, providing greater visibility of lane markers.
- **Infrared sensor array:** Five infrared sensors detecting black lane boundary markers, indicating vehicle proximity to lane edges.

## Model Design

The architecture closely mirrors the previous phase:

### Image Preprocessing and Encoding (VAE)

- **Raw images:** 160×128 pixels.
- **Preprocessing:** Cropped to 160×80 pixels (top removed).
- **Latent encoding:** 32-dimensional compact vector via pretrained VAE, trained specifically on the real-world images.

### State Representation

- 32-dimensional latent vector from the VAE.
- History of the 2 most recent actions (steering, acceleration), totaling 4 additional values.
- **Total state input:** 36-dimensional vector for SAC networks.

### Action Space

Continuous actions:

- **Steering angle**
- **Acceleration**

Despite having small electric motors that can execute commands almost immediately, I emulated the control of a road vehicle. The agent doesn't set the speed of the motors. It sets the acceleration that will increase or decrease the actual speed. Also, steering changes are incrementally limited (±15% per tick) to produce smoother trajectories and immitate a real road vehicle.

## SAC Architecture

- **Input Layer:** 36 neurons (32 latent vector + 4 recent actions).
- **Hidden Layers:** Two layers (32 and 16 neurons).
- **Output:**
  - Actor: 2 neurons (steering, acceleration).
  - Critic: 1 neuron (predicted state-action value).
- **Activation Function:** ELU for hidden layers, linear output.

## Reward Model

Reward function encourages lane-centered, stable driving:

- **Base reward:** +1 per timestep within lane boundaries.
- **Acceleration bonus:** Proportional to normalized acceleration, weight = 0.1.
- **Penalty:** -10 with additional penalty based on acceleration if lane boundary detected by infrared sensors.

### Experimental Stages and Results

#### 1. Straight Corridor Navigation
- **Objective:** Teach the vehicle to drive straight down a corridor without crashing.
- **Challenges:** Initially, the vehicle learned poorly if it always started from the same spot.
- **Improvements:** Started training from varied positions and orientations.
- **Results:** Successfully navigated the corridor after only 3 training episodes of 5000 ticks each.

#### 2. Lane-Following Circuit (Initial Configuration)
- **Objective:** Teach the vehicle to follow a lane marked by black and white lines.
- **Challenges:** The vehicle repeatedly failed at curves because the VAE struggled to accurately encode images.
- **Improvements:** Increased the dataset diversity and experimented with a wide-angle camera.
- **Results:** Despite improvements, the vehicle still couldn't consistently complete the circuit.

#### 3. Refined Lane-Following Circuit
- **Objective:** Achieve consistent lane-following by making the environment more similar to the simulator.
- **Challenges:** Previous setups lacked clear lane differentiation and accurate sensor feedback.
- **Improvements:**
  - Distinctly colored lane markings (left side: white/black, right side: red/black).
  - Wide-angle camera for better lane visibility.
  - Infrared sensors to detect lane departures precisely.
  - Increased historical command context from 2 to 20 previous actions.
  - Adjusted acceleration for smoother control.
- **Results:**
  - The vehicle completed the circuit reliably by episode 61.
  - Delaying training start (from tick 600 to 1200) further boosted consistency, achieving success by episode 36.
  - Confirmed each improvement was necessary through systematic tests.


## Conclusions

- Optimized hardware and VAE training significantly improved performance.
- Comprehensive dataset diversity and accurate sensor integration proved essential for robust real-world autonomous driving.
- SAC demonstrated reliable, smooth control in complex real-world scenarios, validating its effectiveness beyond simulation.

This phase represents a major milestone, successfully bridging the gap between simulation and real-world application, and marking the completion of my first complex Machine Learning and Reinforcement Learning project. The knowledge and experience gained throughout this journey have been invaluable, and the project itself has sparked my passion and drive to pursue even greater challenges in this exciting field
